{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xyunyao/structural_evo_breakdown/blob/main/updated_AlphaFold2_SHP2_6CRF_chainA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4yBrceuFbf3"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/sokrypton/ColabFold/main/.github/ColabFold_Marv_Logo_Small.png\" height=\"200\" align=\"right\" style=\"height:240px\">\n",
        "\n",
        "##ColabFold v1.5.5: AlphaFold2 using MMseqs2\n",
        "\n",
        "Easy to use protein structure and complex prediction using [AlphaFold2](https://www.nature.com/articles/s41586-021-03819-2) and [Alphafold2-multimer](https://www.biorxiv.org/content/10.1101/2021.10.04.463034v1). Sequence alignments/templates are generated through [MMseqs2](mmseqs.com) and [HHsearch](https://github.com/soedinglab/hh-suite). For more details, see <a href=\"#Instructions\">bottom</a> of the notebook, checkout the [ColabFold GitHub](https://github.com/sokrypton/ColabFold) and [Nature Protocols](https://www.nature.com/articles/s41596-024-01060-5).\n",
        "\n",
        "Old versions: [v1.4](https://colab.research.google.com/github/sokrypton/ColabFold/blob/v1.4.0/AlphaFold2.ipynb), [v1.5.1](https://colab.research.google.com/github/sokrypton/ColabFold/blob/v1.5.1/AlphaFold2.ipynb), [v1.5.2](https://colab.research.google.com/github/sokrypton/ColabFold/blob/v1.5.2/AlphaFold2.ipynb), [v1.5.3-patch](https://colab.research.google.com/github/sokrypton/ColabFold/blob/56c72044c7d51a311ca99b953a71e552fdc042e1/AlphaFold2.ipynb)\n",
        "\n",
        "[Mirdita M, SchÃ¼tze K, Moriwaki Y, Heo L, Ovchinnikov S, Steinegger M. ColabFold: Making protein folding accessible to all.\n",
        "*Nature Methods*, 2022](https://www.nature.com/articles/s41592-022-01488-1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOblAo-xetgx",
        "cellView": "form",
        "outputId": "61c882a0-0f0b-4455-ef8c-28bec81b9eba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "#@title Input protein sequence(s), then hit `Runtime` -> `Run all`\n",
        "from google.colab import files\n",
        "import os\n",
        "import re\n",
        "import hashlib\n",
        "import random\n",
        "\n",
        "from sys import version_info\n",
        "python_version = f\"{version_info.major}.{version_info.minor}\"\n",
        "\n",
        "def add_hash(x,y):\n",
        "  return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]\n",
        "\n",
        "query_sequence = 'MTSRRWFHPNITGVEAENLLLTRGVDGSFLARPSKSNPGDFTLSVRRNGAVTHIKIQNTGDYYDLYGGEKFATLAELVQY YMEHHGQLKEKNGDVIELKYPLNCADPTSERWFHGHLSGKEAEKLLTEKGKHGSFLVRESQSHPGDFVLSVRTGDDKGES NDGKSKVTHVMIRCQELKYDVGGGERFDSLTDLVEHYKKNPMVETLGTVLQLKQPLNTTRINAAEIESRVRELSKLAETT DKVKQGFWEEFETLQQQECKLLYSRKEGQRQENKNKNRYKNILPFDHTRVVLHDGDPNEPVSDYINANIIMPEFETKCNN SKPKKSYIATQGCLQNTVNDFWRMVFQENSRVIVMTTKEVERGKSKCVKYWPDEYALKEYGVMRVRNVKESAAHDYTLRE LKLSKVGQGNTERTVWQYHFRTWPDHGVPSDPGGVLDFLEEVHHKQESIMDAGPVVVHCSAGIGRTGTFIVIDILIDIIR EKGVDCDIDVPKTIQMVRSQRSGMVQTEAQYRFIYMAVQHYIETLQRRIEEEQKSKRKGHEYTNIKYSLADQTSGDQSPL PPCTPTPPCAEMREDSARVYENVGLMQQQKSFR' #@param {type:\"string\"}\n",
        "#@markdown  - Use `:` to specify inter-protein chainbreaks for **modeling complexes** (supports homo- and hetro-oligomers). For example **PI...SK:PI...SK** for a homodimer\n",
        "jobname = 'SHP2_run2' #@param {type:\"string\"}\n",
        "# number of models to use\n",
        "num_relax = 1 #@param [0, 1, 5] {type:\"raw\"}\n",
        "#@markdown - specify how many of the top ranked structures to relax using amber\n",
        "template_mode = \"custom\" #@param [\"none\", \"pdb100\",\"custom\"]\n",
        "#@markdown - `none` = no template information is used. `pdb100` = detect templates in pdb100 (see [notes](#pdb100)). `custom` - upload and search own templates (PDB or mmCIF format, see [notes](#custom_templates))\n",
        "\n",
        "use_amber = num_relax > 0\n",
        "\n",
        "# remove whitespaces\n",
        "query_sequence = \"\".join(query_sequence.split())\n",
        "\n",
        "basejobname = \"\".join(jobname.split())\n",
        "basejobname = re.sub(r'\\W+', '', basejobname)\n",
        "jobname = add_hash(basejobname, query_sequence)\n",
        "\n",
        "# check if directory with jobname exists\n",
        "def check(folder):\n",
        "  if os.path.exists(folder):\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "if not check(jobname):\n",
        "  n = 0\n",
        "  while not check(f\"{jobname}_{n}\"): n += 1\n",
        "  jobname = f\"{jobname}_{n}\"\n",
        "\n",
        "# make directory to save results\n",
        "os.makedirs(jobname, exist_ok=True)\n",
        "\n",
        "# save queries\n",
        "queries_path = os.path.join(jobname, f\"{jobname}.csv\")\n",
        "with open(queries_path, \"w\") as text_file:\n",
        "  text_file.write(f\"id,sequence\\n{jobname},{query_sequence}\")\n",
        "\n",
        "if template_mode == \"pdb100\":\n",
        "  use_templates = True\n",
        "  custom_template_path = None\n",
        "elif template_mode == \"custom\":\n",
        "  custom_template_path = os.path.join(jobname,f\"template\")\n",
        "  os.makedirs(custom_template_path, exist_ok=True)\n",
        "  uploaded = files.upload()\n",
        "  use_templates = True\n",
        "  for fn in uploaded.keys():\n",
        "    os.rename(fn,os.path.join(custom_template_path,fn))\n",
        "else:\n",
        "  custom_template_path = None\n",
        "  use_templates = False\n",
        "\n",
        "print(\"jobname\",jobname)\n",
        "print(\"sequence\",query_sequence)\n",
        "print(\"length\",len(query_sequence.replace(\":\",\"\")))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-646f5a31-d8c3-4ea3-8513-350ab10b4a41\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-646f5a31-d8c3-4ea3-8513-350ab10b4a41\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 6crf_chainA.pdb to 6crf_chainA.pdb\n",
            "jobname SHP2_run2_235a9\n",
            "sequence MTSRRWFHPNITGVEAENLLLTRGVDGSFLARPSKSNPGDFTLSVRRNGAVTHIKIQNTGDYYDLYGGEKFATLAELVQYYMEHHGQLKEKNGDVIELKYPLNCADPTSERWFHGHLSGKEAEKLLTEKGKHGSFLVRESQSHPGDFVLSVRTGDDKGESNDGKSKVTHVMIRCQELKYDVGGGERFDSLTDLVEHYKKNPMVETLGTVLQLKQPLNTTRINAAEIESRVRELSKLAETTDKVKQGFWEEFETLQQQECKLLYSRKEGQRQENKNKNRYKNILPFDHTRVVLHDGDPNEPVSDYINANIIMPEFETKCNNSKPKKSYIATQGCLQNTVNDFWRMVFQENSRVIVMTTKEVERGKSKCVKYWPDEYALKEYGVMRVRNVKESAAHDYTLRELKLSKVGQGNTERTVWQYHFRTWPDHGVPSDPGGVLDFLEEVHHKQESIMDAGPVVVHCSAGIGRTGTFIVIDILIDIIREKGVDCDIDVPKTIQMVRSQRSGMVQTEAQYRFIYMAVQHYIETLQRRIEEEQKSKRKGHEYTNIKYSLADQTSGDQSPLPPCTPTPPCAEMREDSARVYENVGLMQQQKSFR\n",
            "length 593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install dependencies\n",
        "%%time\n",
        "import os\n",
        "\n",
        "USE_AMBER = use_amber\n",
        "USE_TEMPLATES = use_templates\n",
        "PYTHON_VERSION = python_version\n",
        "\n",
        "if not os.path.isfile(\"COLABFOLD_READY\"):\n",
        "  print(\"installing colabfold...\")\n",
        "\n",
        "  # install TensorFlow compatible with Python 3.12\n",
        "  os.system(\"pip install -q --no-warn-conflicts tensorflow==2.16.1\")\n",
        "\n",
        "  # install ColabFold (without TPU/JAX conflicts)\n",
        "  os.system(\"pip install -q --no-warn-conflicts 'colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold'\")\n",
        "\n",
        "  # fix TPU import issue (create dummy module if TPU not available)\n",
        "  with open(\"tpu_info.py\", \"w\") as f:\n",
        "    f.write(\"\"\"class device:\n",
        "    @staticmethod\n",
        "    def get_local_chips():\n",
        "        return []\"\"\")\n",
        "\n",
        "  # install JAX for CUDA12 (works on Colab T4 GPU)\n",
        "  os.system(\"pip install -q --no-warn-conflicts 'jax[cuda12]' jaxlib\")\n",
        "\n",
        "  os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/colabfold colabfold\")\n",
        "  os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/alphafold alphafold\")\n",
        "  os.system(\"touch COLABFOLD_READY\")\n",
        "\n",
        "if USE_AMBER or USE_TEMPLATES:\n",
        "  if not os.path.isfile(\"CONDA_READY\"):\n",
        "    print(\"installing conda...\")\n",
        "    os.system(\"wget -qnc https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh\")\n",
        "    os.system(\"bash Miniforge3-Linux-x86_64.sh -bfp /usr/local\")\n",
        "    os.system(\"mamba config --set auto_update_conda false\")\n",
        "    os.system(\"touch CONDA_READY\")\n",
        "\n",
        "if USE_TEMPLATES and not os.path.isfile(\"HH_READY\") and USE_AMBER and not os.path.isfile(\"AMBER_READY\"):\n",
        "  print(\"installing hhsuite and amber...\")\n",
        "  os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 openmm=8.2.0 python='{PYTHON_VERSION}' pdbfixer\")\n",
        "  os.system(\"touch HH_READY\")\n",
        "  os.system(\"touch AMBER_READY\")\n",
        "else:\n",
        "  if USE_TEMPLATES and not os.path.isfile(\"HH_READY\"):\n",
        "    print(\"installing hhsuite...\")\n",
        "    os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 python='{PYTHON_VERSION}'\")\n",
        "    os.system(\"touch HH_READY\")\n",
        "  if USE_AMBER and not os.path.isfile(\"AMBER_READY\"):\n",
        "    print(\"installing amber...\")\n",
        "    os.system(f\"mamba install -y -c conda-forge openmm=8.2.0 python='{PYTHON_VERSION}' pdbfixer\")\n",
        "    os.system(\"touch AMBER_READY\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AzIKiDiCaHAn",
        "outputId": "38d37a5b-9cea-4863-9e61-5084c5b7f159",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "installing colabfold...\n",
            "installing conda...\n",
            "installing hhsuite and amber...\n",
            "CPU times: user 4.41 ms, sys: 3.89 ms, total: 8.3 ms\n",
            "Wall time: 1min 16s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### MSA options (custom MSA upload, single sequence, pairing mode)\n",
        "msa_mode = \"mmseqs2_uniref_env\" #@param [\"mmseqs2_uniref_env\", \"mmseqs2_uniref\",\"single_sequence\",\"custom\"]\n",
        "pair_mode = \"unpaired_paired\" #@param [\"unpaired_paired\",\"paired\",\"unpaired\"] {type:\"string\"}\n",
        "#@markdown - \"unpaired_paired\" = pair sequences from same species + unpaired MSA, \"unpaired\" = seperate MSA for each chain, \"paired\" - only use paired sequences.\n",
        "\n",
        "# decide which a3m to use\n",
        "if \"mmseqs2\" in msa_mode:\n",
        "  a3m_file = os.path.join(jobname,f\"{jobname}.a3m\")\n",
        "\n",
        "elif msa_mode == \"custom\":\n",
        "  a3m_file = os.path.join(jobname,f\"{jobname}.custom.a3m\")\n",
        "  if not os.path.isfile(a3m_file):\n",
        "    custom_msa_dict = files.upload()\n",
        "    custom_msa = list(custom_msa_dict.keys())[0]\n",
        "    header = 0\n",
        "    import fileinput\n",
        "    for line in fileinput.FileInput(custom_msa,inplace=1):\n",
        "      if line.startswith(\">\"):\n",
        "         header = header + 1\n",
        "      if not line.rstrip():\n",
        "        continue\n",
        "      if line.startswith(\">\") == False and header == 1:\n",
        "         query_sequence = line.rstrip()\n",
        "      print(line, end='')\n",
        "\n",
        "    os.rename(custom_msa, a3m_file)\n",
        "    queries_path=a3m_file\n",
        "    print(f\"moving {custom_msa} to {a3m_file}\")\n",
        "\n",
        "else:\n",
        "  a3m_file = os.path.join(jobname,f\"{jobname}.single_sequence.a3m\")\n",
        "  with open(a3m_file, \"w\") as text_file:\n",
        "    text_file.write(\">1\\n%s\" % query_sequence)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "C2_sh2uAonJH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydrive2"
      ],
      "metadata": {
        "id": "jv5ppA6EOgoW",
        "outputId": "318ad7c4-6b23-437c-ff3b-46da3e198acd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydrive2\n",
            "  Downloading PyDrive2-1.21.3-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting google-api-python-client>=1.12.5 (from pydrive2)\n",
            "  Downloading google_api_python_client-2.181.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting oauth2client>=4.0.0 (from pydrive2)\n",
            "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting PyYAML>=3.0 (from pydrive2)\n",
            "  Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting cryptography<44 (from pydrive2)\n",
            "  Downloading cryptography-43.0.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting pyOpenSSL<=24.2.1,>=19.1.0 (from pydrive2)\n",
            "  Downloading pyOpenSSL-24.2.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/site-packages (from cryptography<44->pydrive2) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/site-packages (from cffi>=1.12->cryptography<44->pydrive2) (2.22)\n",
            "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client>=1.12.5->pydrive2)\n",
            "  Downloading httplib2-0.30.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 (from google-api-python-client>=1.12.5->pydrive2)\n",
            "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client>=1.12.5->pydrive2)\n",
            "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 (from google-api-python-client>=1.12.5->pydrive2)\n",
            "  Downloading google_api_core-2.25.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client>=1.12.5->pydrive2)\n",
            "  Downloading uritemplate-4.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.12.5->pydrive2)\n",
            "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.12.5->pydrive2)\n",
            "  Downloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
            "Collecting proto-plus<2.0.0,>=1.22.3 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.12.5->pydrive2)\n",
            "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.12.5->pydrive2) (2.32.4)\n",
            "Collecting cachetools<6.0,>=2.0.0 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client>=1.12.5->pydrive2)\n",
            "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client>=1.12.5->pydrive2)\n",
            "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client>=1.12.5->pydrive2)\n",
            "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting pyparsing<4,>=3.0.4 (from httplib2<1.0.0,>=0.19.0->google-api-python-client>=1.12.5->pydrive2)\n",
            "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.12.5->pydrive2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.12.5->pydrive2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.12.5->pydrive2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.12.5->pydrive2) (2025.7.14)\n",
            "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client>=1.12.5->pydrive2)\n",
            "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting six>=1.6.1 (from oauth2client>=4.0.0->pydrive2)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Downloading PyDrive2-1.21.3-py3-none-any.whl (47 kB)\n",
            "Downloading cryptography-43.0.3-cp39-abi3-manylinux_2_28_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyOpenSSL-24.2.1-py3-none-any.whl (58 kB)\n",
            "Downloading google_api_python_client-2.181.0-py3-none-any.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m157.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_api_core-2.25.1-py3-none-any.whl (160 kB)\n",
            "Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
            "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
            "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
            "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
            "Downloading httplib2-0.30.0-py3-none-any.whl (91 kB)\n",
            "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
            "Downloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\n",
            "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
            "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
            "Downloading uritemplate-4.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
            "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
            "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
            "Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m767.5/767.5 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: uritemplate, six, PyYAML, pyparsing, pyasn1, protobuf, cachetools, rsa, pyasn1-modules, proto-plus, httplib2, googleapis-common-protos, cryptography, pyOpenSSL, oauth2client, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, pydrive2\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m20/20\u001b[0m [pydrive2]\n",
            "\u001b[1A\u001b[2KSuccessfully installed PyYAML-6.0.2 cachetools-5.5.2 cryptography-43.0.3 google-api-core-2.25.1 google-api-python-client-2.181.0 google-auth-2.40.3 google-auth-httplib2-0.2.0 googleapis-common-protos-1.70.0 httplib2-0.30.0 oauth2client-4.1.3 proto-plus-1.26.1 protobuf-6.32.0 pyOpenSSL-24.2.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 pydrive2-1.21.3 pyparsing-3.2.3 rsa-4.9.1 six-1.17.0 uritemplate-4.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### Advanced settings\n",
        "model_type = \"auto\" #@param [\"auto\", \"alphafold2_ptm\", \"alphafold2_multimer_v1\", \"alphafold2_multimer_v2\", \"alphafold2_multimer_v3\", \"deepfold_v1\", \"alphafold2\"]\n",
        "#@markdown - if `auto` selected, will use `alphafold2_ptm` for monomer prediction and `alphafold2_multimer_v3` for complex prediction.\n",
        "#@markdown Any of the mode_types can be used (regardless if input is monomer or complex).\n",
        "num_recycles = \"3\" #@param [\"auto\", \"0\", \"1\", \"3\", \"6\", \"12\", \"24\", \"48\"]\n",
        "#@markdown - if `auto` selected, will use `num_recycles=20` if `model_type=alphafold2_multimer_v3`, else `num_recycles=3` .\n",
        "recycle_early_stop_tolerance = \"auto\" #@param [\"auto\", \"0.0\", \"0.5\", \"1.0\"]\n",
        "#@markdown - if `auto` selected, will use `tol=0.5` if `model_type=alphafold2_multimer_v3` else `tol=0.0`.\n",
        "relax_max_iterations = 200 #@param [0, 200, 2000] {type:\"raw\"}\n",
        "#@markdown - max amber relax iterations, `0` = unlimited (AlphaFold2 default, can take very long)\n",
        "pairing_strategy = \"greedy\" #@param [\"greedy\", \"complete\"] {type:\"string\"}\n",
        "#@markdown - `greedy` = pair any taxonomically matching subsets, `complete` = all sequences have to match in one line.\n",
        "calc_extra_ptm = False #@param {type:\"boolean\"}\n",
        "#@markdown - return pairwise chain iptm/actifptm\n",
        "\n",
        "#@markdown #### Sample settings\n",
        "#@markdown -  enable dropouts and increase number of seeds to sample predictions from uncertainty of the model.\n",
        "#@markdown -  decrease `max_msa` to increase uncertainity\n",
        "max_msa = \"auto\" #@param [\"auto\", \"512:1024\", \"256:512\", \"64:128\", \"32:64\", \"16:32\"]\n",
        "num_seeds = 1 #@param [1,2,4,8,16] {type:\"raw\"}\n",
        "use_dropout = True #@param {type:\"boolean\"}\n",
        "\n",
        "num_recycles = None if num_recycles == \"auto\" else int(num_recycles)\n",
        "recycle_early_stop_tolerance = None if recycle_early_stop_tolerance == \"auto\" else float(recycle_early_stop_tolerance)\n",
        "if max_msa == \"auto\": max_msa = None\n",
        "\n",
        "#@markdown #### Save settings\n",
        "save_all = False #@param {type:\"boolean\"}\n",
        "save_recycles = True #@param {type:\"boolean\"}\n",
        "save_to_google_drive = True #@param {type:\"boolean\"}\n",
        "#@markdown -  if the save_to_google_drive option was selected, the result zip will be uploaded to your Google Drive\n",
        "dpi = 200 #@param {type:\"integer\"}\n",
        "#@markdown - set dpi for image resolution\n",
        "\n",
        "if save_to_google_drive:\n",
        "  from pydrive2.drive import GoogleDrive\n",
        "  from pydrive2.auth import GoogleAuth\n",
        "  from google.colab import auth\n",
        "  from oauth2client.client import GoogleCredentials\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "  print(\"You are logged into Google Drive and are good to go!\")\n",
        "\n",
        "#@markdown Don't forget to hit `Runtime` -> `Run all` after updating the form."
      ],
      "metadata": {
        "cellView": "form",
        "id": "ADDuaolKmjGW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f65763f6-d5db-45ba-c3f6-34303fadf267"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are logged into Google Drive and are good to go!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U colabfold tensorflow==2.16.1 \"jax[cuda12]\" jaxlib\n",
        "\n",
        "# Create a dummy tpu_info.py to bypass TPU check\n",
        "import os\n",
        "with open(\"tpu_info.py\", \"w\") as f:\n",
        "    f.write(\"\"\"class device:\n",
        "    @staticmethod\n",
        "    def get_local_chips():\n",
        "        return []\"\"\")\n"
      ],
      "metadata": {
        "id": "f8wXODBuRGR4",
        "outputId": "0cbe1d04-d5ca-4d1c-8156-fbf5be06462f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting colabfold\n",
            "  Downloading colabfold-1.5.4-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting tensorflow==2.16.1\n",
            "  Downloading tensorflow-2.16.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Collecting jaxlib\n",
            "  Downloading jaxlib-0.7.1-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting jax[cuda12]\n",
            "  Downloading jax-0.7.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting absl-py>=1.0.0 (from tensorflow==2.16.1)\n",
            "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow==2.16.1)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=23.5.26 (from tensorflow==2.16.1)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow==2.16.1)\n",
            "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow==2.16.1)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting h5py>=3.10.0 (from tensorflow==2.16.1)\n",
            "  Downloading h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Collecting libclang>=13.0.0 (from tensorflow==2.16.1)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow==2.16.1)\n",
            "  Downloading ml_dtypes-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting opt-einsum>=2.3.2 (from tensorflow==2.16.1)\n",
            "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.1) (25.0)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.16.1)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.1) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.1) (80.9.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.1) (1.17.0)\n",
            "Collecting termcolor>=1.1.0 (from tensorflow==2.16.1)\n",
            "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting typing-extensions>=3.6.6 (from tensorflow==2.16.1)\n",
            "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting wrapt>=1.11.0 (from tensorflow==2.16.1)\n",
            "  Downloading wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow==2.16.1)\n",
            "  Downloading grpcio-1.74.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow==2.16.1)\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting keras>=3.0.0 (from tensorflow==2.16.1)\n",
            "  Downloading keras-3.11.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting numpy<2.0.0,>=1.26.0 (from tensorflow==2.16.1)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (2025.7.14)\n",
            "Collecting markdown>=2.6.8 (from tensorboard<2.17,>=2.16->tensorflow==2.16.1)\n",
            "  Downloading markdown-3.9-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow==2.16.1)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard<2.17,>=2.16->tensorflow==2.16.1)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting absl-py>=1.0.0 (from tensorflow==2.16.1)\n",
            "  Downloading absl_py-1.4.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting appdirs<2.0.0,>=1.4.4 (from colabfold)\n",
            "  Using cached appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting biopython<=1.82 (from colabfold)\n",
            "  Downloading biopython-1.82-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting dm-haiku<0.0.12,>=0.0.11 (from colabfold)\n",
            "  Downloading dm_haiku-0.0.11-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting importlib-metadata<5.0.0,>=4.8.2 (from colabfold)\n",
            "  Downloading importlib_metadata-4.13.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting matplotlib<4.0.0,>=3.2.2 (from colabfold)\n",
            "  Downloading matplotlib-3.10.6-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting pandas<2.0.0,>=1.3.4 (from colabfold)\n",
            "  Downloading pandas-1.5.3.tar.gz (5.2 MB)\n",
            "\u001b[2K     \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py3Dmol<3.0.0,>=2.0.1 (from colabfold)\n",
            "  Using cached py3dmol-2.5.2-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting tensorflow-cpu<3.0.0,>=2.12.1 (from colabfold)\n",
            "  Using cached tensorflow_cpu-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.62.2 in /usr/local/lib/python3.12/site-packages (from colabfold) (4.67.1)\n",
            "Collecting jmp>=0.0.2 (from dm-haiku<0.0.12,>=0.0.11->colabfold)\n",
            "  Using cached jmp-0.0.4-py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting tabulate>=0.8.9 (from dm-haiku<0.0.12,>=0.0.11->colabfold)\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting flax>=0.7.1 (from dm-haiku<0.0.12,>=0.0.11->colabfold)\n",
            "  Downloading flax-0.11.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting zipp>=0.5 (from importlib-metadata<5.0.0,>=4.8.2->colabfold)\n",
            "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib<4.0.0,>=3.2.2->colabfold)\n",
            "  Downloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib<4.0.0,>=3.2.2->colabfold)\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib<4.0.0,>=3.2.2->colabfold)\n",
            "  Downloading fonttools-4.59.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (109 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib<4.0.0,>=3.2.2->colabfold)\n",
            "  Downloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting pillow>=8 (from matplotlib<4.0.0,>=3.2.2->colabfold)\n",
            "  Downloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/site-packages (from matplotlib<4.0.0,>=3.2.2->colabfold) (3.2.3)\n",
            "Collecting python-dateutil>=2.7 (from matplotlib<4.0.0,>=3.2.2->colabfold)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas<2.0.0,>=1.3.4->colabfold)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "INFO: pip is looking at multiple versions of tensorflow-cpu to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow-cpu<3.0.0,>=2.12.1 (from colabfold)\n",
            "  Downloading tensorflow_cpu-2.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "  Downloading tensorflow_cpu-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "  Downloading tensorflow_cpu-2.18.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "  Downloading tensorflow_cpu-2.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "  Downloading tensorflow_cpu-2.17.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "  Downloading tensorflow_cpu-2.17.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "  Downloading tensorflow_cpu-2.16.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "INFO: pip is looking at multiple versions of jax[cuda12] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax[cuda12]\n",
            "  Downloading jax-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib\n",
            "  Downloading jaxlib-0.7.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting jax[cuda12]\n",
            "  Downloading jax-0.6.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib\n",
            "  Downloading jaxlib-0.6.2-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting jax[cuda12]\n",
            "  Downloading jax-0.6.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib\n",
            "  Downloading jaxlib-0.6.1-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax[cuda12]\n",
            "  Downloading jax-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib\n",
            "  Downloading jaxlib-0.6.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax[cuda12]\n",
            "  Downloading jax-0.5.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib\n",
            "  Downloading jaxlib-0.5.3-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax[cuda12]\n",
            "  Downloading jax-0.5.2-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib\n",
            "  Downloading jaxlib-0.5.1-cp312-cp312-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
            "Collecting jax[cuda12]\n",
            "  Downloading jax-0.5.1-py3-none-any.whl.metadata (22 kB)\n",
            "INFO: pip is still looking at multiple versions of jax[cuda12] to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading jax-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib\n",
            "  Downloading jaxlib-0.5.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
            "Collecting jax[cuda12]\n",
            "  Downloading jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib\n",
            "  Downloading jaxlib-0.4.38-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jax[cuda12]\n",
            "  Downloading jax-0.4.37-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib\n",
            "  Downloading jaxlib-0.4.36-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jax[cuda12]\n",
            "  Downloading jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib\n",
            "  Downloading jaxlib-0.4.35-cp312-cp312-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting jax[cuda12]\n",
            "  Downloading jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib\n",
            "  Downloading jaxlib-0.4.34-cp312-cp312-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.12/site-packages (from jax[cuda12]) (1.16.1)\n",
            "Collecting jax-cuda12-plugin<=0.4.34,>=0.4.34 (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading jax_cuda12_plugin-0.4.34-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax-cuda12-pjrt==0.4.34 (from jax-cuda12-plugin<=0.4.34,>=0.4.34->jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading jax_cuda12_pjrt-0.4.34-py3-none-manylinux2014_x86_64.whl.metadata (349 bytes)\n",
            "Collecting nvidia-cublas-cu12>=12.1.3.1 (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading nvidia_cublas_cu12-12.9.1.4-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12>=12.1.105 (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.9.79-py3-none-manylinux_2_25_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cuda-nvcc-cu12>=12.1.105 (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading nvidia_cuda_nvcc_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12>=12.1.105 (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.9.79-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cudnn-cu12<10.0,>=9.1 (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading nvidia_cudnn_cu12-9.13.0.50-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cufft-cu12>=11.0.2.54 (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading nvidia_cufft_cu12-11.4.1.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusolver-cu12>=11.4.5.107 (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading nvidia_cusolver_cu12-11.7.5.82-py3-none-manylinux_2_27_x86_64.whl.metadata (1.9 kB)\n",
            "Collecting nvidia-cusparse-cu12>=12.1.0.106 (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading nvidia_cusparse_cu12-12.5.10.65-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nccl-cu12>=2.18.1 (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading nvidia_nccl_cu12-2.28.3-py3-none-manylinux_2_18_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvjitlink-cu12>=12.1.105 (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow==2.16.1) (0.45.1)\n",
            "INFO: pip is looking at multiple versions of flax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting flax>=0.7.1 (from dm-haiku<0.0.12,>=0.0.11->colabfold)\n",
            "  Downloading flax-0.11.1-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading flax-0.11.0-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading flax-0.10.7-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading flax-0.10.6-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading flax-0.10.5-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading flax-0.10.4-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting msgpack (from flax>=0.7.1->dm-haiku<0.0.12,>=0.0.11->colabfold)\n",
            "  Downloading msgpack-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting optax (from flax>=0.7.1->dm-haiku<0.0.12,>=0.0.11->colabfold)\n",
            "  Downloading optax-0.2.5-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting orbax-checkpoint (from flax>=0.7.1->dm-haiku<0.0.12,>=0.0.11->colabfold)\n",
            "  Downloading orbax_checkpoint-0.11.24-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting tensorstore (from flax>=0.7.1->dm-haiku<0.0.12,>=0.0.11->colabfold)\n",
            "  Downloading tensorstore-0.1.76-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting rich>=11.1 (from flax>=0.7.1->dm-haiku<0.0.12,>=0.0.11->colabfold)\n",
            "  Downloading rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.12/site-packages (from flax>=0.7.1->dm-haiku<0.0.12,>=0.0.11->colabfold) (6.0.2)\n",
            "Collecting treescope>=0.1.7 (from flax>=0.7.1->dm-haiku<0.0.12,>=0.0.11->colabfold)\n",
            "  Downloading treescope-0.1.10-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting namex (from keras>=3.0.0->tensorflow==2.16.1)\n",
            "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
            "Collecting optree (from keras>=3.0.0->tensorflow==2.16.1)\n",
            "  Downloading optree-0.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (33 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=11.1->flax>=0.7.1->dm-haiku<0.0.12,>=0.0.11->colabfold)\n",
            "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting pygments<3.0.0,>=2.13.0 (from rich>=11.1->flax>=0.7.1->dm-haiku<0.0.12,>=0.0.11->colabfold)\n",
            "  Downloading pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.1->dm-haiku<0.0.12,>=0.0.11->colabfold)\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.1)\n",
            "  Downloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting chex>=0.1.87 (from optax->flax>=0.7.1->dm-haiku<0.0.12,>=0.0.11->colabfold)\n",
            "  Downloading chex-0.1.91-py3-none-any.whl.metadata (18 kB)\n",
            "INFO: pip is looking at multiple versions of chex to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading chex-0.1.90-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting toolz>=0.9.0 (from chex>=0.1.87->optax->flax>=0.7.1->dm-haiku<0.0.12,>=0.0.11->colabfold)\n",
            "  Downloading toolz-1.0.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting etils[epath,epy] (from orbax-checkpoint->flax>=0.7.1->dm-haiku<0.0.12,>=0.0.11->colabfold)\n",
            "  Downloading etils-1.13.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "INFO: pip is looking at multiple versions of orbax-checkpoint to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting orbax-checkpoint (from flax>=0.7.1->dm-haiku<0.0.12,>=0.0.11->colabfold)\n",
            "  Downloading orbax_checkpoint-0.11.23-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading orbax_checkpoint-0.11.22-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading orbax_checkpoint-0.11.21-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading orbax_checkpoint-0.11.20-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading orbax_checkpoint-0.11.19-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading orbax_checkpoint-0.11.18-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading orbax_checkpoint-0.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
            "INFO: pip is still looking at multiple versions of orbax-checkpoint to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading orbax_checkpoint-0.11.16-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.11.15-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.11.14-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.11.13-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.11.12-py3-none-any.whl.metadata (2.0 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading orbax_checkpoint-0.11.11-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.11.10-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.11.9-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.11.8-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.11.7-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.11.6-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading orbax_checkpoint-0.11.5-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting nest_asyncio (from orbax-checkpoint->flax>=0.7.1->dm-haiku<0.0.12,>=0.0.11->colabfold)\n",
            "  Downloading nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting humanize (from orbax-checkpoint->flax>=0.7.1->dm-haiku<0.0.12,>=0.0.11->colabfold)\n",
            "  Downloading humanize-4.13.0-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting simplejson>=3.16.0 (from orbax-checkpoint->flax>=0.7.1->dm-haiku<0.0.12,>=0.0.11->colabfold)\n",
            "  Downloading simplejson-3.20.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "INFO: pip is looking at multiple versions of tensorstore to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorstore (from flax>=0.7.1->dm-haiku<0.0.12,>=0.0.11->colabfold)\n",
            "  Downloading tensorstore-0.1.75-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "  Downloading tensorstore-0.1.74-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting fsspec (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.1->dm-haiku<0.0.12,>=0.0.11->colabfold)\n",
            "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting importlib_resources (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.1->dm-haiku<0.0.12,>=0.0.11->colabfold)\n",
            "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Downloading tensorflow-2.16.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.9 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m589.9/589.9 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.74.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m128.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m156.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m129.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m138.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colabfold-1.5.4-py3-none-any.whl (65 kB)\n",
            "Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
            "Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading biopython-1.82-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dm_haiku-0.0.11-py3-none-any.whl (370 kB)\n",
            "Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
            "Downloading matplotlib-3.10.6-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m109.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached py3dmol-2.5.2-py2.py3-none-any.whl (7.2 kB)\n",
            "Downloading tensorflow_cpu-2.16.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (214.2 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m214.2/214.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax-0.4.34-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.4.34-cp312-cp312-manylinux2014_x86_64.whl (86.2 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m86.2/86.2 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax_cuda12_plugin-0.4.34-cp312-cp312-manylinux2014_x86_64.whl (14.9 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax_cuda12_pjrt-0.4.34-py3-none-manylinux2014_x86_64.whl (100.3 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m100.3/100.3 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.13.0.50-py3-none-manylinux_2_27_x86_64.whl (644.2 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m644.2/644.2 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
            "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading flax-0.10.4-py3-none-any.whl (451 kB)\n",
            "Downloading fonttools-4.59.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Downloading h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m128.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached jmp-0.0.4-py3-none-any.whl (18 kB)\n",
            "Downloading keras-3.11.3-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m168.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown-3.9-py3-none-any.whl (107 kB)\n",
            "Downloading nvidia_cublas_cu12-12.9.1.4-py3-none-manylinux_2_27_x86_64.whl (581.2 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m581.2/581.2 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.9.79-py3-none-manylinux_2_25_x86_64.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvcc_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (40.5 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.9.79-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.4.1.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.9 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m200.9/200.9 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.5.82-py3-none-manylinux_2_27_x86_64.whl (338.1 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m338.1/338.1 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.10.65-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (366.5 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m366.5/366.5 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.28.3-py3-none-manylinux_2_18_x86_64.whl (295.9 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m295.9/295.9 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "Downloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Downloading rich-14.1.0-py3-none-any.whl (243 kB)\n",
            "Downloading pygments-2.19.2-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
            "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
            "Downloading treescope-0.1.10-py3-none-any.whl (182 kB)\n",
            "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "Downloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Downloading wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (88 kB)\n",
            "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
            "Downloading msgpack-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n",
            "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
            "Downloading optax-0.2.5-py3-none-any.whl (354 kB)\n",
            "Downloading chex-0.1.90-py3-none-any.whl (101 kB)\n",
            "Downloading toolz-1.0.0-py3-none-any.whl (56 kB)\n",
            "Downloading optree-0.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (408 kB)\n",
            "Downloading orbax_checkpoint-0.11.5-py3-none-any.whl (342 kB)\n",
            "Downloading simplejson-3.20.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
            "Downloading tensorstore-0.1.74-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.4 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m18.4/18.4 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading etils-1.13.0-py3-none-any.whl (170 kB)\n",
            "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
            "Downloading humanize-4.13.0-py3-none-any.whl (128 kB)\n",
            "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
            "Downloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
            "Building wheels for collected packages: pandas\n",
            "  Building wheel for pandas (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandas: filename=pandas-1.5.3-cp312-cp312-linux_x86_64.whl size=11476525 sha256=8b669c731fea29018de254e35d0b23aafaaa692fb9eeca2328b202d6114fc76e\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/83/18/8e7307aa1185c5498c5490e4d9c8a1732d9f1056e86c3491c6\n",
            "Successfully built pandas\n",
            "Installing collected packages: pytz, py3Dmol, namex, libclang, jax-cuda12-pjrt, flatbuffers, appdirs, zipp, wrapt, typing-extensions, toolz, termcolor, tensorboard-data-server, tabulate, simplejson, python-dateutil, pygments, protobuf, pillow, opt-einsum, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvcc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, nest_asyncio, msgpack, mdurl, MarkupSafe, markdown, kiwisolver, jax-cuda12-plugin, importlib_resources, humanize, grpcio, google-pasta, gast, fsspec, fonttools, etils, cycler, astunparse, absl-py, werkzeug, treescope, pandas, optree, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, ml-dtypes, markdown-it-py, jmp, importlib-metadata, h5py, contourpy, biopython, tensorstore, tensorboard, rich, nvidia-cusolver-cu12, matplotlib, jaxlib, keras, jax, tensorflow-cpu, tensorflow, orbax-checkpoint, chex, optax, flax, dm-haiku, colabfold\n",
            "\u001b[2K  Attempting uninstall: protobuf\n",
            "\u001b[2K    Found existing installation: protobuf 6.32.0\n",
            "\u001b[2K    Uninstalling protobuf-6.32.0:\n",
            "\u001b[2K      Successfully uninstalled protobuf-6.32.0\n",
            "\u001b[2K  Attempting uninstall: numpy\n",
            "\u001b[2K    Found existing installation: numpy 2.3.2\n",
            "\u001b[2K    Uninstalling numpy-2.3.2:\n",
            "\u001b[2K      Successfully uninstalled numpy-2.3.2\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m75/75\u001b[0m [colabfold]\n",
            "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 absl-py-1.4.0 appdirs-1.4.4 astunparse-1.6.3 biopython-1.82 chex-0.1.90 colabfold-1.5.4 contourpy-1.3.3 cycler-0.12.1 dm-haiku-0.0.11 etils-1.13.0 flatbuffers-25.2.10 flax-0.10.4 fonttools-4.59.2 fsspec-2025.9.0 gast-0.6.0 google-pasta-0.2.0 grpcio-1.74.0 h5py-3.14.0 humanize-4.13.0 importlib-metadata-4.13.0 importlib_resources-6.5.2 jax-0.4.34 jax-cuda12-pjrt-0.4.34 jax-cuda12-plugin-0.4.34 jaxlib-0.4.34 jmp-0.0.4 keras-3.11.3 kiwisolver-1.4.9 libclang-18.1.1 markdown-3.9 markdown-it-py-4.0.0 matplotlib-3.10.6 mdurl-0.1.2 ml-dtypes-0.3.2 msgpack-1.1.1 namex-0.1.0 nest_asyncio-1.6.0 numpy-1.26.4 nvidia-cublas-cu12-12.9.1.4 nvidia-cuda-cupti-cu12-12.9.79 nvidia-cuda-nvcc-cu12-12.9.86 nvidia-cuda-runtime-cu12-12.9.79 nvidia-cudnn-cu12-9.13.0.50 nvidia-cufft-cu12-11.4.1.4 nvidia-cusolver-cu12-11.7.5.82 nvidia-cusparse-cu12-12.5.10.65 nvidia-nccl-cu12-2.28.3 nvidia-nvjitlink-cu12-12.9.86 opt-einsum-3.4.0 optax-0.2.5 optree-0.17.0 orbax-checkpoint-0.11.5 pandas-1.5.3 pillow-11.3.0 protobuf-4.25.8 py3Dmol-2.5.2 pygments-2.19.2 python-dateutil-2.9.0.post0 pytz-2025.2 rich-14.1.0 simplejson-3.20.1 tabulate-0.9.0 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-cpu-2.16.2 tensorstore-0.1.74 termcolor-3.1.0 toolz-1.0.0 treescope-0.1.10 typing-extensions-4.15.0 werkzeug-3.1.3 wrapt-1.17.3 zipp-3.23.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Prediction\n",
        "display_images = True #@param {type:\"boolean\"}\n",
        "\n",
        "import sys\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "from Bio import BiopythonDeprecationWarning\n",
        "warnings.simplefilter(action='ignore', category=BiopythonDeprecationWarning)\n",
        "from pathlib import Path\n",
        "from colabfold.download import download_alphafold_params, default_data_dir\n",
        "from colabfold.utils import setup_logging\n",
        "from colabfold.batch import get_queries, run, set_model_type\n",
        "from colabfold.plot import plot_msa_v2\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "try:\n",
        "  K80_chk = os.popen('nvidia-smi | grep \"Tesla K80\" | wc -l').read()\n",
        "except:\n",
        "  K80_chk = \"0\"\n",
        "  pass\n",
        "if \"1\" in K80_chk:\n",
        "  print(\"WARNING: found GPU Tesla K80: limited to total length < 1000\")\n",
        "  if \"TF_FORCE_UNIFIED_MEMORY\" in os.environ:\n",
        "    del os.environ[\"TF_FORCE_UNIFIED_MEMORY\"]\n",
        "  if \"XLA_PYTHON_CLIENT_MEM_FRACTION\" in os.environ:\n",
        "    del os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]\n",
        "\n",
        "from colabfold.colabfold import plot_protein\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# For some reason we need that to get pdbfixer to import\n",
        "if use_amber and f\"/usr/local/lib/python{python_version}/site-packages/\" not in sys.path:\n",
        "    sys.path.insert(0, f\"/usr/local/lib/python{python_version}/site-packages/\")\n",
        "\n",
        "def input_features_callback(input_features):\n",
        "  if display_images:\n",
        "    plot_msa_v2(input_features)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def prediction_callback(protein_obj, length,\n",
        "                        prediction_result, input_features, mode):\n",
        "  model_name, relaxed = mode\n",
        "  if not relaxed:\n",
        "    if display_images:\n",
        "      fig = plot_protein(protein_obj, Ls=length, dpi=150)\n",
        "      plt.show()\n",
        "      plt.close()\n",
        "\n",
        "result_dir = jobname\n",
        "log_filename = os.path.join(jobname,\"log.txt\")\n",
        "setup_logging(Path(log_filename))\n",
        "\n",
        "queries, is_complex = get_queries(queries_path)\n",
        "model_type = set_model_type(is_complex, model_type)\n",
        "\n",
        "if \"multimer\" in model_type and max_msa is not None:\n",
        "  use_cluster_profile = False\n",
        "else:\n",
        "  use_cluster_profile = True\n",
        "\n",
        "download_alphafold_params(model_type, Path(\".\"))\n",
        "results = run(\n",
        "    queries=queries,\n",
        "    result_dir=result_dir,\n",
        "    use_templates=use_templates,\n",
        "    custom_template_path=custom_template_path,\n",
        "    num_relax=num_relax,\n",
        "    msa_mode=msa_mode,\n",
        "    model_type=model_type,\n",
        "    num_models=5,\n",
        "    num_recycles=num_recycles,\n",
        "    relax_max_iterations=relax_max_iterations,\n",
        "    recycle_early_stop_tolerance=recycle_early_stop_tolerance,\n",
        "    num_seeds=num_seeds,\n",
        "    use_dropout=use_dropout,\n",
        "    model_order=[1,2,3,4,5],\n",
        "    is_complex=is_complex,\n",
        "    data_dir=Path(\".\"),\n",
        "    keep_existing_results=False,\n",
        "    rank_by=\"auto\",\n",
        "    pair_mode=pair_mode,\n",
        "    pairing_strategy=pairing_strategy,\n",
        "    stop_at_score=float(100),\n",
        "    prediction_callback=prediction_callback,\n",
        "    dpi=dpi,\n",
        "    zip_results=False,\n",
        "    save_all=save_all,\n",
        "    max_msa=max_msa,\n",
        "    use_cluster_profile=use_cluster_profile,\n",
        "    input_features_callback=input_features_callback,\n",
        "    save_recycles=save_recycles,\n",
        "    user_agent=\"colabfold/google-colab-main\",\n",
        "    calc_extra_ptm=calc_extra_ptm,\n",
        ")\n",
        "results_zip = f\"{jobname}.result.zip\"\n",
        "os.system(f\"zip -r {results_zip} {jobname}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mbaIO9pWjaN0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "outputId": "c662bcf0-677d-4106-d2e3-917d5c251c5a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading alphafold2_ptm weights to .: 100%|ââââââââââ| 3.47G/3.47G [00:22<00:00, 163MB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "/usr/local/lib/python3.12/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow15TensorShapeBaseINS_11TensorShapeEEC2EN4absl12lts_202308024SpanIKlEE",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/content/colabfold/batch.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(queries, result_dir, num_models, is_complex, num_recycles, recycle_early_stop_tolerance, model_order, initial_guess, num_ensemble, model_type, msa_mode, use_templates, custom_template_path, num_relax, relax_max_iterations, relax_tolerance, relax_stiffness, relax_max_outer_iterations, keep_existing_results, rank_by, pair_mode, pairing_strategy, data_dir, host_url, user_agent, random_seed, num_seeds, recompile_padding, zip_results, prediction_callback, save_single_representations, save_pair_representations, jobname_prefix, save_all, save_recycles, use_dropout, use_gpu_relax, stop_at_score, dpi, max_seq, max_extra_seq, pdb_hit_file, local_pdb_path, use_cluster_profile, feature_dict_callback, calc_extra_ptm, use_probs_extra, **kwargs)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;31m# check if TPU is available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtpu_info\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_chips\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tpu_info'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1348602437.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mdownload_alphafold_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m results = run(\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mqueries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mresult_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresult_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/colabfold/batch.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(queries, result_dir, num_models, is_complex, num_recycles, recycle_early_stop_tolerance, model_order, initial_guess, num_ensemble, model_type, msa_mode, use_templates, custom_template_path, num_relax, relax_max_iterations, relax_tolerance, relax_stiffness, relax_max_outer_iterations, keep_existing_results, rank_by, pair_mode, pairing_strategy, data_dir, host_url, user_agent, random_seed, num_seeds, recompile_padding, zip_results, prediction_callback, save_single_representations, save_pair_representations, jobname_prefix, save_all, save_recycles, use_dropout, use_gpu_relax, stop_at_score, dpi, max_seq, max_extra_seq, pdb_hit_file, local_pdb_path, use_cluster_profile, feature_dict_callback, calc_extra_ptm, use_probs_extra, **kwargs)\u001b[0m\n\u001b[1;32m   1125\u001b[0m             \u001b[0muse_gpu_relax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m             \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1128\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running on GPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    430\u001b[0m   \u001b[0m_kernel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_tf_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"kernels\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_kernel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m     \u001b[0m_ll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_kernel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m   \u001b[0;31m# Load third party dynamic kernels.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/load_library.py\u001b[0m in \u001b[0;36mload_library\u001b[0;34m(library_location)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlib\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkernel_libraries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m       \u001b[0mpy_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_LoadLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: /usr/local/lib/python3.12/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow15TensorShapeBaseINS_11TensorShapeEEC2EN4absl12lts_202308024SpanIKlEE"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK7X9T44pWb7",
        "cellView": "form"
      },
      "source": [
        "#@title Display 3D structure {run: \"auto\"}\n",
        "import py3Dmol\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from colabfold.colabfold import plot_plddt_legend\n",
        "from colabfold.colabfold import pymol_color_list, alphabet_list\n",
        "rank_num = 1 #@param [\"1\", \"2\", \"3\", \"4\", \"5\"] {type:\"raw\"}\n",
        "color = \"lDDT\" #@param [\"chain\", \"lDDT\", \"rainbow\"]\n",
        "show_sidechains = False #@param {type:\"boolean\"}\n",
        "show_mainchains = False #@param {type:\"boolean\"}\n",
        "\n",
        "tag = results[\"rank\"][0][rank_num - 1]\n",
        "jobname_prefix = \".custom\" if msa_mode == \"custom\" else \"\"\n",
        "pdb_filename = f\"{jobname}/{jobname}{jobname_prefix}_unrelaxed_{tag}.pdb\"\n",
        "pdb_file = glob.glob(pdb_filename)\n",
        "\n",
        "def show_pdb(rank_num=1, show_sidechains=False, show_mainchains=False, color=\"lDDT\"):\n",
        "  model_name = f\"rank_{rank_num}\"\n",
        "  view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js',)\n",
        "  view.addModel(open(pdb_file[0],'r').read(),'pdb')\n",
        "\n",
        "  if color == \"lDDT\":\n",
        "    view.setStyle({'cartoon': {'colorscheme': {'prop':'b','gradient': 'roygb','min':50,'max':90}}})\n",
        "  elif color == \"rainbow\":\n",
        "    view.setStyle({'cartoon': {'color':'spectrum'}})\n",
        "  elif color == \"chain\":\n",
        "    chains = len(queries[0][1]) + 1 if is_complex else 1\n",
        "    for n,chain,color in zip(range(chains),alphabet_list,pymol_color_list):\n",
        "       view.setStyle({'chain':chain},{'cartoon': {'color':color}})\n",
        "\n",
        "  if show_sidechains:\n",
        "    BB = ['C','O','N']\n",
        "    view.addStyle({'and':[{'resn':[\"GLY\",\"PRO\"],'invert':True},{'atom':BB,'invert':True}]},\n",
        "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "    view.addStyle({'and':[{'resn':\"GLY\"},{'atom':'CA'}]},\n",
        "                        {'sphere':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "    view.addStyle({'and':[{'resn':\"PRO\"},{'atom':['C','O'],'invert':True}]},\n",
        "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "  if show_mainchains:\n",
        "    BB = ['C','O','N','CA']\n",
        "    view.addStyle({'atom':BB},{'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "\n",
        "  view.zoomTo()\n",
        "  return view\n",
        "\n",
        "show_pdb(rank_num, show_sidechains, show_mainchains, color).show()\n",
        "if color == \"lDDT\":\n",
        "  plot_plddt_legend().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11l8k--10q0C",
        "cellView": "form"
      },
      "source": [
        "#@title Plots {run: \"auto\"}\n",
        "from IPython.display import display, HTML\n",
        "import base64\n",
        "from html import escape\n",
        "\n",
        "# see: https://stackoverflow.com/a/53688522\n",
        "def image_to_data_url(filename):\n",
        "  ext = filename.split('.')[-1]\n",
        "  prefix = f'data:image/{ext};base64,'\n",
        "  with open(filename, 'rb') as f:\n",
        "    img = f.read()\n",
        "  return prefix + base64.b64encode(img).decode('utf-8')\n",
        "\n",
        "pae = \"\"\n",
        "pae_file = os.path.join(jobname,f\"{jobname}{jobname_prefix}_pae.png\")\n",
        "if os.path.isfile(pae_file):\n",
        "    pae = image_to_data_url(pae_file)\n",
        "cov = image_to_data_url(os.path.join(jobname,f\"{jobname}{jobname_prefix}_coverage.png\"))\n",
        "plddt = image_to_data_url(os.path.join(jobname,f\"{jobname}{jobname_prefix}_plddt.png\"))\n",
        "display(HTML(f\"\"\"\n",
        "<style>\n",
        "  img {{\n",
        "    float:left;\n",
        "  }}\n",
        "  .full {{\n",
        "    max-width:100%;\n",
        "  }}\n",
        "  .half {{\n",
        "    max-width:50%;\n",
        "  }}\n",
        "  @media (max-width:640px) {{\n",
        "    .half {{\n",
        "      max-width:100%;\n",
        "    }}\n",
        "  }}\n",
        "</style>\n",
        "<div style=\"max-width:90%; padding:2em;\">\n",
        "  <h1>Plots for {escape(jobname)}</h1>\n",
        "  { '<!--' if pae == '' else '' }<img src=\"{pae}\" class=\"full\" />{ '-->' if pae == '' else '' }\n",
        "  <img src=\"{cov}\" class=\"half\" />\n",
        "  <img src=\"{plddt}\" class=\"half\" />\n",
        "</div>\n",
        "\"\"\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Package and download results\n",
        "#@markdown If you are having issues downloading the result archive, try disabling your adblocker and run this cell again. If that fails click on the little folder icon to the left, navigate to file: `jobname.result.zip`, right-click and select \\\"Download\\\" (see [screenshot](https://pbs.twimg.com/media/E6wRW2lWUAEOuoe?format=jpg&name=small)).\n",
        "\n",
        "if msa_mode == \"custom\":\n",
        "  print(\"Don't forget to cite your custom MSA generation method.\")\n",
        "\n",
        "files.download(f\"{jobname}.result.zip\")\n",
        "\n",
        "if save_to_google_drive == True and drive:\n",
        "  uploaded = drive.CreateFile({'title': f\"{jobname}.result.zip\"})\n",
        "  uploaded.SetContentFile(f\"{jobname}.result.zip\")\n",
        "  uploaded.Upload()\n",
        "  print(f\"Uploaded {jobname}.result.zip to Google Drive with ID {uploaded.get('id')}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "R_AH6JSXaeb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGUBLzB3C6WN",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Instructions <a name=\"Instructions\"></a>\n",
        "For detailed instructions, tips and tricks, see recently published paper at [Nature Protocols](https://www.nature.com/articles/s41596-024-01060-5)\n",
        "\n",
        "**Quick start**\n",
        "1. Paste your protein sequence(s) in the input field.\n",
        "2. Press \"Runtime\" -> \"Run all\".\n",
        "3. The pipeline consists of 5 steps. The currently running step is indicated by a circle with a stop sign next to it.\n",
        "\n",
        "**Result zip file contents**\n",
        "\n",
        "1. PDB formatted structures sorted by avg. pLDDT and complexes are sorted by pTMscore. (unrelaxed and relaxed if `use_amber` is enabled).\n",
        "2. Plots of the model quality.\n",
        "3. Plots of the MSA coverage.\n",
        "4. Parameter log file.\n",
        "5. A3M formatted input MSA.\n",
        "6. A `predicted_aligned_error_v1.json` using [AlphaFold-DB's format](https://alphafold.ebi.ac.uk/faq#faq-7) and a `scores.json` for each model which contains an array (list of lists) for PAE, a list with the average pLDDT and the pTMscore.\n",
        "7. BibTeX file with citations for all used tools and databases.\n",
        "\n",
        "At the end of the job a download modal box will pop up with a `jobname.result.zip` file. Additionally, if the `save_to_google_drive` option was selected, the `jobname.result.zip` will be uploaded to your Google Drive.\n",
        "\n",
        "**MSA generation for complexes**\n",
        "\n",
        "For the complex prediction we use unpaired and paired MSAs. Unpaired MSA is generated the same way as for the protein structures prediction by searching the UniRef100 and environmental sequences three iterations each.\n",
        "\n",
        "The paired MSA is generated by searching the UniRef100 database and pairing the best hits sharing the same NCBI taxonomic identifier (=species or sub-species). We only pair sequences if all of the query sequences are present for the respective taxonomic identifier.\n",
        "\n",
        "**Using a custom MSA as input**\n",
        "\n",
        "To predict the structure with a custom MSA (A3M formatted): (1) Change the `msa_mode`: to \"custom\", (2) Wait for an upload box to appear at the end of the \"MSA options ...\" box. Upload your A3M. The first fasta entry of the A3M must be the query sequence without gaps.\n",
        "\n",
        "It is also possilbe to provide custom MSAs for complex predictions. Read more about the format [here](https://github.com/sokrypton/ColabFold/issues/76).\n",
        "\n",
        "As an alternative for MSA generation the [HHblits Toolkit server](https://toolkit.tuebingen.mpg.de/tools/hhblits) can be used. After submitting your query, click \"Query Template MSA\" -> \"Download Full A3M\". Download the A3M file and upload it in this notebook.\n",
        "\n",
        "**PDB100** <a name=\"pdb100\"></a>\n",
        "\n",
        "As of 23/06/08, we have transitioned from using the PDB70 to a 100% clustered PDB, the PDB100. The construction methodology of PDB100 differs from that of PDB70.\n",
        "\n",
        "The PDB70 was constructed by running each PDB70 representative sequence through [HHblits](https://github.com/soedinglab/hh-suite) against the [Uniclust30](https://uniclust.mmseqs.com/). On the other hand, the PDB100 is built by searching each PDB100 representative structure with [Foldseek](https://github.com/steineggerlab/foldseek) against the [AlphaFold Database](https://alphafold.ebi.ac.uk).\n",
        "\n",
        "To maintain compatibility with older Notebook versions and local installations, the generated files and API responses will continue to be named \"PDB70\", even though we're now using the PDB100.\n",
        "\n",
        "**Using custom templates** <a name=\"custom_templates\"></a>\n",
        "\n",
        "To predict the structure with a custom template (PDB or mmCIF formatted): (1) change the `template_mode` to \"custom\" in the execute cell and (2) wait for an upload box to appear at the end of the \"Input Protein\" box. Select and upload your templates (multiple choices are possible).\n",
        "\n",
        "* Templates must follow the four letter PDB naming with lower case letters.\n",
        "\n",
        "* Templates in mmCIF format must contain `_entity_poly_seq`. An error is thrown if this field is not present. The field `_pdbx_audit_revision_history.revision_date` is automatically generated if it is not present.\n",
        "\n",
        "* Templates in PDB format are automatically converted to the mmCIF format. `_entity_poly_seq` and `_pdbx_audit_revision_history.revision_date` are automatically generated.\n",
        "\n",
        "If you encounter problems, please report them to this [issue](https://github.com/sokrypton/ColabFold/issues/177).\n",
        "\n",
        "**Comparison to the full AlphaFold2 and AlphaFold2 Colab**\n",
        "\n",
        "This notebook replaces the homology detection and MSA pairing of AlphaFold2 with MMseqs2. For a comparison against the [AlphaFold2 Colab](https://colab.research.google.com/github/deepmind/alphafold/blob/main/notebooks/AlphaFold.ipynb) and the full [AlphaFold2](https://github.com/deepmind/alphafold) system read our [paper](https://www.nature.com/articles/s41592-022-01488-1).\n",
        "\n",
        "**Troubleshooting**\n",
        "* Check that the runtime type is set to GPU at \"Runtime\" -> \"Change runtime type\".\n",
        "* Try to restart the session \"Runtime\" -> \"Factory reset runtime\".\n",
        "* Check your input sequence.\n",
        "\n",
        "**Known issues**\n",
        "* Google Colab assigns different types of GPUs with varying amount of memory. Some might not have enough memory to predict the structure for a long sequence.\n",
        "* Your browser can block the pop-up for downloading the result file. You can choose the `save_to_google_drive` option to upload to Google Drive instead or manually download the result file: Click on the little folder icon to the left, navigate to file: `jobname.result.zip`, right-click and select \\\"Download\\\" (see [screenshot](https://pbs.twimg.com/media/E6wRW2lWUAEOuoe?format=jpg&name=small)).\n",
        "\n",
        "**Limitations**\n",
        "* Computing resources: Our MMseqs2 API can handle ~20-50k requests per day.\n",
        "* MSAs: MMseqs2 is very precise and sensitive but might find less hits compared to HHblits/HMMer searched against BFD or MGnify.\n",
        "* We recommend to additionally use the full [AlphaFold2 pipeline](https://github.com/deepmind/alphafold).\n",
        "\n",
        "**Description of the plots**\n",
        "*   **Number of sequences per position** - We want to see at least 30 sequences per position, for best performance, ideally 100 sequences.\n",
        "*   **Predicted lDDT per position** - model confidence (out of 100) at each position. The higher the better.\n",
        "*   **Predicted Alignment Error** - For homooligomers, this could be a useful metric to assess how confident the model is about the interface. The lower the better.\n",
        "\n",
        "**Bugs**\n",
        "- If you encounter any bugs, please report the issue to https://github.com/sokrypton/ColabFold/issues\n",
        "\n",
        "**License**\n",
        "\n",
        "The source code of ColabFold is licensed under [MIT](https://raw.githubusercontent.com/sokrypton/ColabFold/main/LICENSE). Additionally, this notebook uses the AlphaFold2 source code and its parameters licensed under [Apache 2.0](https://raw.githubusercontent.com/deepmind/alphafold/main/LICENSE) and [CC BY 4.0](https://creativecommons.org/licenses/by-sa/4.0/) respectively. Read more about the AlphaFold license [here](https://github.com/deepmind/alphafold).\n",
        "\n",
        "**Acknowledgments**\n",
        "- We thank the AlphaFold team for developing an excellent model and open sourcing the software.\n",
        "\n",
        "- [KOBIC](https://kobic.re.kr) and [SÃ¶ding Lab](https://www.mpinat.mpg.de/soeding) for providing the computational resources for the MMseqs2 MSA server.\n",
        "\n",
        "- Richard Evans for helping to benchmark the ColabFold's Alphafold-multimer support.\n",
        "\n",
        "- [David Koes](https://github.com/dkoes) for his awesome [py3Dmol](https://3dmol.csb.pitt.edu/) plugin, without whom these notebooks would be quite boring!\n",
        "\n",
        "- Do-Yoon Kim for creating the ColabFold logo.\n",
        "\n",
        "- A colab by Sergey Ovchinnikov ([@sokrypton](https://twitter.com/sokrypton)), Milot Mirdita ([@milot_mirdita](https://twitter.com/milot_mirdita)) and Martin Steinegger ([@thesteinegger](https://twitter.com/thesteinegger)).\n"
      ]
    }
  ]
}