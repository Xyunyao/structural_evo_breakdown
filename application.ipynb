{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7537753",
   "metadata": {},
   "source": [
    "# This is a notebook to show cases of ESM_if"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a387c0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yunyao/miniconda3/envs/struct-evo/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/yunyao/miniconda3/envs/struct-evo/lib/python3.9/site-packages/esm/pretrained.py:215: UserWarning: Regression weights not found, predicting contacts will not produce correct results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import esm\n",
    "#load the model\n",
    "model, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\n",
    "model = model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffeb0407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-06-13 12:39:53--  https://files.rcsb.org/download/7mmo.cif\n",
      "Connecting to 128.59.114.167:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/octet-stream]\n",
      "Saving to: ‘data/7mmo.cif.1’\n",
      "\n",
      "7mmo.cif.1              [ <=>                ] 968.49K  --.-KB/s    in 0.05s   \n",
      "\n",
      "2025-06-13 12:39:54 (19.8 MB/s) - ‘data/7mmo.cif.1’ saved [991730]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download the example structure\n",
    "!wget https://files.rcsb.org/download/7mmo.cif -P data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f004d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "841d0081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Native sequence:\n",
      "ITLKESGPTLVKPTQTLTLTCTFSGFSLSISGVGVGWLRQPPGKALEWLALIYWDDDKRYSPSLKSRLTISKDTSKNQVVLKMTNIDPVDTATYYCAHHSISTIFDHWGQGTLVTVSSASTKGPSVFPLAPCTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTKTYTCNVDHKPSNTKVDKRVHH\n"
     ]
    }
   ],
   "source": [
    "# information on structure to be designed\n",
    "fpath = 'data/7mmo.cif' # .pdb format is also acceptable\n",
    "chain_id = 'A'\n",
    "structure = esm.inverse_folding.util.load_structure(fpath, chain_id)\n",
    "coords, native_seq = esm.inverse_folding.util.extract_coords_from_structure(structure)\n",
    "print('Native sequence:')\n",
    "print(native_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03463df5",
   "metadata": {},
   "source": [
    "# Application 1. sample new seqence based on the backbone structure\n",
    "\n",
    "The sample strategy follows a multinomial sampling fashion, which samples based on probility distribution and temperature. (lower probability will be also sampled, espically at high temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "679e9aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled sequence: STNTVSGPTVVKPTDTLTLQCTYSGFSLNTTGVGLGWSRTPQGSTWEELALIHWNNEKTYKPSEKADLFVSKNDVKQTVTLKYTHLQPSDTAVYYCAYYTGETVMTNFSAGLQVTVSSATIQGPSVLPLTPGTVVVGCAINDFYPQPVTVTVNSGSLTSGVTVYPQVLLPSGLYKRNGLVTVPTSSCNTTTITFNVAHKPSDTTVNQVVNC\n",
      "Sequence recovery: 0.5308056872037915\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sampled_seq = model.sample(coords, temperature=1)\n",
    "print('Sampled sequence:', sampled_seq)\n",
    "\n",
    "recovery = np.mean([(a==b) for a, b in zip(native_seq, sampled_seq)])\n",
    "print('Sequence recovery:', recovery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efc93e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked coordinates shape: (211, 3, 3)\n",
      "average log-likelihood on entire sequence: -1.44 (perplexity 4.22)\n",
      "average log-likelihood excluding missing coordinates: -1.33 (perplexity 3.77)\n",
      "Sampled sequence with masked coordinates: GPADAPQVQLVVRQGQLSLLCEYSGFELKTAGVGIGFRWRPPGKSEEGLALILHDDTTYYNPSLRSRLAVSSNVQKKTVTLVMSEVVPQDTATYYCGFVTGTHQVTSWSPGVLVIVSKAKPTGPKVMPLKPGELTLGCSIESYWPESVTVSWNSGTTARGVTIQPSQLLPTGLYKRDGTVTVPKSRCDTVSYTCNVLHVPTATQVALLVHC\n",
      "Sequence recovery: 0.45023696682464454\n"
     ]
    }
   ],
   "source": [
    "# we can also sample sequences conditioned on a partial structure\n",
    "from copy import deepcopy\n",
    "masked_coords = deepcopy(coords)\n",
    "print('Masked coordinates shape:', masked_coords.shape)\n",
    "masked_coords[:15] = np.inf # mask the first 15 residues\n",
    "ll_fullseq, ll_withcoord = esm.inverse_folding.util.score_sequence(model, alphabet, masked_coords, native_seq)\n",
    "\n",
    "print(f'average log-likelihood on entire sequence: {ll_fullseq:.2f} (perplexity {np.exp(-ll_fullseq):.2f})')\n",
    "print(f'average log-likelihood excluding missing coordinates: {ll_withcoord:.2f} (perplexity {np.exp(-ll_withcoord):.2f})')\n",
    "\n",
    "sampled_seq = model.sample(masked_coords, temperature=1)\n",
    "print('Sampled sequence with masked coordinates:', sampled_seq)\n",
    "\n",
    "recovery = np.mean([(a==b) for a, b in zip(native_seq, sampled_seq)])\n",
    "print('Sequence recovery:', recovery)\n",
    "\n",
    "# we can also sample sequences conditioned on a partial sequence\n",
    "# masked_seq = deepcopy(native_seq)\n",
    "# masked_seq[:15] = '-' # mask the first 15 residues\n",
    "# ll_fullseq, ll_withseq = esm.inverse_folding.util.score_sequence(model, alphabet, coords, masked_seq)\n",
    "# print(f'average log-likelihood on entire sequence: {ll_fullseq:.2f} (perplexity {np.exp(-ll_fullseq):.2f})')\n",
    "# print(f'average log-likelihood excluding missing sequence: {ll_withseq:.2f} (perplexity {np.exp(-ll_withseq):.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "475b67d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method GVPTransformerModel.forward of GVPTransformerModel(\n",
      "  (encoder): GVPTransformerEncoder(\n",
      "    (dropout_module): Dropout(p=0.1, inplace=False)\n",
      "    (embed_tokens): Embedding(35, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (embed_gvp_input_features): Linear(in_features=15, out_features=512, bias=True)\n",
      "    (embed_confidence): Linear(in_features=16, out_features=512, bias=True)\n",
      "    (embed_dihedrals): DihedralFeatures(\n",
      "      (node_embedding): Linear(in_features=6, out_features=512, bias=True)\n",
      "      (norm_nodes): Normalize()\n",
      "    )\n",
      "    (gvp_encoder): GVPEncoder(\n",
      "      (embed_graph): GVPGraphEmbedding(\n",
      "        (embed_node): Sequential(\n",
      "          (0): GVP(\n",
      "            (wh): Linear(in_features=3, out_features=256, bias=False)\n",
      "            (ws): Linear(in_features=263, out_features=1024, bias=True)\n",
      "            (wv): Linear(in_features=256, out_features=256, bias=False)\n",
      "          )\n",
      "          (1): LayerNorm(\n",
      "            (scalar_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (embed_edge): Sequential(\n",
      "          (0): GVP(\n",
      "            (wh): Linear(in_features=1, out_features=1, bias=False)\n",
      "            (ws): Linear(in_features=35, out_features=32, bias=True)\n",
      "            (wv): Linear(in_features=1, out_features=1, bias=False)\n",
      "          )\n",
      "          (1): LayerNorm(\n",
      "            (scalar_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (embed_confidence): Linear(in_features=16, out_features=1024, bias=True)\n",
      "      )\n",
      "      (encoder_layers): ModuleList(\n",
      "        (0): GVPConvLayer(\n",
      "          (conv): GVPConv()\n",
      "          (norm): ModuleList(\n",
      "            (0): LayerNorm(\n",
      "              (scalar_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): LayerNorm(\n",
      "              (scalar_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (dropout): ModuleList(\n",
      "            (0): Dropout(\n",
      "              (sdropout): Dropout(p=0.1, inplace=False)\n",
      "              (vdropout): _VDropout()\n",
      "            )\n",
      "            (1): Dropout(\n",
      "              (sdropout): Dropout(p=0.1, inplace=False)\n",
      "              (vdropout): _VDropout()\n",
      "            )\n",
      "          )\n",
      "          (ff_func): Sequential(\n",
      "            (0): GVP(\n",
      "              (wh): Linear(in_features=256, out_features=512, bias=False)\n",
      "              (ws): Linear(in_features=1536, out_features=4096, bias=True)\n",
      "              (wv): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (wg): Linear(in_features=4096, out_features=512, bias=True)\n",
      "            )\n",
      "            (1): GVP(\n",
      "              (wh): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (ws): Linear(in_features=4608, out_features=1024, bias=True)\n",
      "              (wv): Linear(in_features=512, out_features=256, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): GVPConvLayer(\n",
      "          (conv): GVPConv()\n",
      "          (norm): ModuleList(\n",
      "            (0): LayerNorm(\n",
      "              (scalar_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): LayerNorm(\n",
      "              (scalar_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (dropout): ModuleList(\n",
      "            (0): Dropout(\n",
      "              (sdropout): Dropout(p=0.1, inplace=False)\n",
      "              (vdropout): _VDropout()\n",
      "            )\n",
      "            (1): Dropout(\n",
      "              (sdropout): Dropout(p=0.1, inplace=False)\n",
      "              (vdropout): _VDropout()\n",
      "            )\n",
      "          )\n",
      "          (ff_func): Sequential(\n",
      "            (0): GVP(\n",
      "              (wh): Linear(in_features=256, out_features=512, bias=False)\n",
      "              (ws): Linear(in_features=1536, out_features=4096, bias=True)\n",
      "              (wv): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (wg): Linear(in_features=4096, out_features=512, bias=True)\n",
      "            )\n",
      "            (1): GVP(\n",
      "              (wh): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (ws): Linear(in_features=4608, out_features=1024, bias=True)\n",
      "              (wv): Linear(in_features=512, out_features=256, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): GVPConvLayer(\n",
      "          (conv): GVPConv()\n",
      "          (norm): ModuleList(\n",
      "            (0): LayerNorm(\n",
      "              (scalar_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): LayerNorm(\n",
      "              (scalar_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (dropout): ModuleList(\n",
      "            (0): Dropout(\n",
      "              (sdropout): Dropout(p=0.1, inplace=False)\n",
      "              (vdropout): _VDropout()\n",
      "            )\n",
      "            (1): Dropout(\n",
      "              (sdropout): Dropout(p=0.1, inplace=False)\n",
      "              (vdropout): _VDropout()\n",
      "            )\n",
      "          )\n",
      "          (ff_func): Sequential(\n",
      "            (0): GVP(\n",
      "              (wh): Linear(in_features=256, out_features=512, bias=False)\n",
      "              (ws): Linear(in_features=1536, out_features=4096, bias=True)\n",
      "              (wv): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (wg): Linear(in_features=4096, out_features=512, bias=True)\n",
      "            )\n",
      "            (1): GVP(\n",
      "              (wh): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (ws): Linear(in_features=4608, out_features=1024, bias=True)\n",
      "              (wv): Linear(in_features=512, out_features=256, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): GVPConvLayer(\n",
      "          (conv): GVPConv()\n",
      "          (norm): ModuleList(\n",
      "            (0): LayerNorm(\n",
      "              (scalar_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): LayerNorm(\n",
      "              (scalar_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (dropout): ModuleList(\n",
      "            (0): Dropout(\n",
      "              (sdropout): Dropout(p=0.1, inplace=False)\n",
      "              (vdropout): _VDropout()\n",
      "            )\n",
      "            (1): Dropout(\n",
      "              (sdropout): Dropout(p=0.1, inplace=False)\n",
      "              (vdropout): _VDropout()\n",
      "            )\n",
      "          )\n",
      "          (ff_func): Sequential(\n",
      "            (0): GVP(\n",
      "              (wh): Linear(in_features=256, out_features=512, bias=False)\n",
      "              (ws): Linear(in_features=1536, out_features=4096, bias=True)\n",
      "              (wv): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (wg): Linear(in_features=4096, out_features=512, bias=True)\n",
      "            )\n",
      "            (1): GVP(\n",
      "              (wh): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (ws): Linear(in_features=4608, out_features=1024, bias=True)\n",
      "              (wv): Linear(in_features=512, out_features=256, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (embed_gvp_output): Linear(in_features=1792, out_features=512, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (6): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (7): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (dropout_module): Dropout(p=0.1, inplace=False)\n",
      "    (embed_tokens): Embedding(35, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (6): TransformerDecoderLayer(\n",
      "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (7): TransformerDecoderLayer(\n",
      "        (dropout_module): Dropout(p=0.1, inplace=False)\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (output_projection): Linear(in_features=512, out_features=35, bias=False)\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(model.forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dec9102",
   "metadata": {},
   "source": [
    "# Application 2: generate structure embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a623b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211, torch.Size([211, 512]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rep = esm.inverse_folding.util.get_encoder_output(model, alphabet, coords)\n",
    "len(coords), rep.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd3d8f4",
   "metadata": {},
   "source": [
    "# Application 3: evaluate squence fitness \n",
    "\n",
    "with the backbone info Y, we evulate the fitness of the sequence X by comparing target = sequence[1:] and predicted ligits based on input seqences =sequence[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "187103e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average log-likelihood on entire sequence: -1.32 (perplexity 3.75)\n",
      "average log-likelihood excluding missing coordinates: -1.32 (perplexity 3.75)\n"
     ]
    }
   ],
   "source": [
    "ll_fullseq, ll_withcoord = esm.inverse_folding.util.score_sequence(model, alphabet, coords, native_seq)\n",
    "print(f'average log-likelihood on entire sequence: {ll_fullseq:.2f} (perplexity {np.exp(-ll_fullseq):.2f})')\n",
    "print(f'average log-likelihood excluding missing coordinates: {ll_withcoord:.2f} (perplexity {np.exp(-ll_withcoord):.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9587f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "dir_path = '/home/yunyao/structural-evolution/bin'\n",
    "\n",
    "# Add the directory containing the modules to sys.path\n",
    "if dir_path not in sys.path:\n",
    "    sys.path.append(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7874a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can test the fitnesss of various sequences generated by dms\n",
    "\n",
    "import argparse\n",
    "from dms_utils import deep_mutational_scan\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import esm\n",
    "from util import load_structure, extract_coords_from_structure\n",
    "import biotite.structure\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def get_native_seq(pdbfile, chain):\n",
    "    structure = load_structure(pdbfile, chain)\n",
    "    _ , native_seq = extract_coords_from_structure(structure)\n",
    "    return native_seq\n",
    "    \n",
    "def write_dms_lib(args):\n",
    "    '''Writes a deep mutational scanning library, including the native/wildtype (wt) of the \n",
    "    indicated target chain in the structure to an output Fasta file'''\n",
    "\n",
    "    sequence = get_native_seq(args.pdbfile, args.chain)\n",
    "    Path(args.dmspath).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(args.dmspath, 'w') as f:\n",
    "        f.write('>wt\\n')\n",
    "        f.write(sequence+'\\n')\n",
    "        for pos, wt, mt in deep_mutational_scan(sequence):\n",
    "            assert(sequence[pos] == wt)\n",
    "            mut_seq = sequence[:pos] + mt + sequence[(pos + 1):]\n",
    "\n",
    "            f.write('>' + str(wt) + str(pos+1) + str(mt) + '\\n')\n",
    "            f.write(mut_seq + '\\n')\n",
    "\n",
    "pdbfile_path= 'data/7mmo.cif'\n",
    "chain_id='A'\n",
    "outpath='/home/yunyao/structural_evo_breakdown/dms_lib'\n",
    "dmspath = f'{outpath}/7mmo_dms_lib.fasta'\n",
    "write_dms_lib(\n",
    "    argparse.Namespace(\n",
    "        pdbfile=pdbfile_path,\n",
    "        chain=chain_id,\n",
    "        dmspath=dmspath,\n",
    "        outpath=outpath\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91ebc061",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # now we can scan through the library and score each sequence\n",
    "dms_lib = defaultdict(list)\n",
    "with open(dmspath, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.startswith('>'):\n",
    "            seq_id = line.strip()[1:]  # remove '>'\n",
    "        else:\n",
    "            sequence = line.strip()\n",
    "            dms_lib[seq_id].append(sequence)\n",
    "# score each sequence in the library\n",
    "scores = {}\n",
    "for seq_id, sequences in dms_lib.items():\n",
    "    for sequence in sequences:\n",
    "        ll_fullseq, ll_withcoord = esm.inverse_folding.util.score_sequence(model, alphabet, coords, sequence)\n",
    "        scores[seq_id] = (ll_fullseq, ll_withcoord)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "struct-evo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
