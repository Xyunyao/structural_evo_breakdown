{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6255f0eb",
   "metadata": {},
   "source": [
    "# Content in util.py code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c91c8e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yunyao/miniconda3/envs/struct-evo/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import biotite.structure\n",
    "from biotite.structure.io import pdbx, pdb\n",
    "from biotite.structure.residues import get_residues\n",
    "from biotite.structure import filter_backbone\n",
    "from biotite.structure import get_chains\n",
    "from biotite.sequence import ProteinSequence\n",
    "import numpy as np\n",
    "from scipy.spatial import transform\n",
    "from scipy.stats import special_ortho_group\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from typing import Sequence, Tuple, List\n",
    "from esm.data import BatchConverter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5adf540",
   "metadata": {},
   "source": [
    "## 1. Structure processing units\n",
    "\n",
    "to extract cooridnates and sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deab369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_structure(fpath, chain=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        fpath: filepath to either pdb or cif file\n",
    "        chain: the chain id or list of chain ids to load\n",
    "    Returns:\n",
    "        biotite.structure.AtomArray\n",
    "    \"\"\"\n",
    "    if fpath.endswith('cif'):\n",
    "        with open(fpath) as fin:\n",
    "            pdbxf = pdbx.PDBxFile.read(fin)\n",
    "        structure = pdbx.get_structure(pdbxf, model=1)\n",
    "    elif fpath.endswith('pdb'):\n",
    "        with open(fpath) as fin:\n",
    "            pdbf = pdb.PDBFile.read(fin)\n",
    "        structure = pdb.get_structure(pdbf, model=1)\n",
    "    bbmask = filter_backbone(structure)\n",
    "    structure = structure[bbmask]\n",
    "    all_chains = get_chains(structure)\n",
    "    if len(all_chains) == 0:\n",
    "        raise ValueError('No chains found in the input file.')\n",
    "    if chain is None:\n",
    "        chain_ids = all_chains\n",
    "    elif isinstance(chain, list):\n",
    "        chain_ids = chain\n",
    "    else:\n",
    "        chain_ids = [chain] \n",
    "    for chain in chain_ids:\n",
    "        if chain not in all_chains:\n",
    "            raise ValueError(f'Chain {chain} not found in input file')\n",
    "    chain_filter = [a.chain_id in chain_ids for a in structure]\n",
    "    structure = structure[chain_filter]\n",
    "    return structure\n",
    "\n",
    "\n",
    "def extract_coords_from_structure(structure: biotite.structure.AtomArray):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        structure: An instance of biotite AtomArray\n",
    "    Returns:\n",
    "        Tuple (coords, seq)\n",
    "            - coords is an L x 3 x 3 array for N, CA, C coordinates\n",
    "            - seq is the extracted sequence\n",
    "    \"\"\"\n",
    "    coords = get_atom_coords_residuewise([\"N\", \"CA\", \"C\"], structure)\n",
    "    residue_identities = get_residues(structure)[1]\n",
    "    seq = ''.join([ProteinSequence.convert_letter_3to1(r) for r in residue_identities])\n",
    "    return coords, seq\n",
    "\n",
    "\n",
    "\n",
    "def get_atom_coords_residuewise(atoms: List[str], struct: biotite.structure.AtomArray):\n",
    "    \"\"\"\n",
    "    Example for atoms argument: [\"N\", \"CA\", \"C\"]\n",
    "    \"\"\"\n",
    "    def filterfn(s, axis=None):\n",
    "        filters = np.stack([s.atom_name == name for name in atoms], axis=1)\n",
    "        sum = filters.sum(0)\n",
    "        if not np.all(sum <= np.ones(filters.shape[1])):\n",
    "            raise RuntimeError(\"structure has multiple atoms with same name\")\n",
    "        index = filters.argmax(0)\n",
    "        coords = s[index].coord\n",
    "        coords[sum == 0] = float(\"nan\")\n",
    "        return coords\n",
    "\n",
    "    return biotite.structure.apply_residue_wise(struct, struct, filterfn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648b4c3a",
   "metadata": {},
   "source": [
    "## 2. reference step using esm and calculate loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6577f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_loss(model, alphabet, coords, seq):\n",
    "    device = next(model.parameters()).device\n",
    "    batch_converter = CoordBatchConverter(alphabet)\n",
    "    batch = [(coords, None, seq)]\n",
    "    coords, confidence, strs, tokens, padding_mask = batch_converter(\n",
    "        batch, device=device)\n",
    "\n",
    "    prev_output_tokens = tokens[:, :-1].to(device)\n",
    "    target = tokens[:, 1:]  # target is shifted by one to the left\n",
    "    target_padding_mask = (target == alphabet.padding_idx)\n",
    "    # model predict next token in an autoregressive manner\n",
    "    logits, _ = model.forward(coords, padding_mask, confidence, prev_output_tokens)\n",
    "    loss = F.cross_entropy(logits, target, reduction='none')\n",
    "    loss = loss[0].cpu().detach().numpy()   # remove bathch dimension\n",
    "    target_padding_mask = target_padding_mask[0].cpu().numpy()\n",
    "    return loss, target_padding_mask  # the output loss is L, token-wise and will be averaged later\n",
    "\n",
    "def score_sequence(model, alphabet, coords, seq):\n",
    "    loss, target_padding_mask = get_sequence_loss(model, alphabet, coords, seq)\n",
    "    # average over non-padding residue\n",
    "    ll_fullseq = -np.sum(loss * ~target_padding_mask) / np.sum(~target_padding_mask)\n",
    "    # Also calculate average when excluding masked portions\n",
    "    # average over non-padding and non-coord residues\n",
    "    coord_mask = np.all(np.isfinite(coords), axis=(-1, -2))\n",
    "    ll_withcoord = -np.sum(loss * coord_mask) / np.sum(coord_mask)\n",
    "    return ll_fullseq, ll_withcoord\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1115520f",
   "metadata": {},
   "source": [
    "## 3. Batch Converter \n",
    "should introduce at step 2  \n",
    "prepare data to feed into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c98208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BatchConverter from ESM\n",
    "# initialized with alphabet from ESM\n",
    "class CoordBatchConverter(BatchConverter):\n",
    "    def __call__(self, raw_batch: Sequence[Tuple[Sequence, str]], device=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            raw_batch: List of tuples (coords, confidence, seq)\n",
    "            In each tuple,\n",
    "                coords: list of floats, shape L x 3 x 3\n",
    "                confidence: list of floats, shape L; or scalar float; or None\n",
    "                seq: string of length L\n",
    "        Returns:\n",
    "            coords: Tensor of shape batch_size x L x 3 x 3\n",
    "            confidence: Tensor of shape batch_size x L\n",
    "            strs: list of strings\n",
    "            tokens: LongTensor of shape batch_size x L\n",
    "            padding_mask: ByteTensor of shape batch_size x L\n",
    "        \"\"\"\n",
    "        self.alphabet.cls_idx = self.alphabet.get_idx(\"<cath>\") \n",
    "        batch = []\n",
    "        for coords, confidence, seq in raw_batch:\n",
    "            if confidence is None:\n",
    "                confidence = 1.\n",
    "            if isinstance(confidence, float) or isinstance(confidence, int):\n",
    "                confidence = [float(confidence)] * len(coords)\n",
    "            if seq is None:\n",
    "                seq = 'X' * len(coords)\n",
    "            batch.append(((coords, confidence), seq))\n",
    "\n",
    "        coords_and_confidence, strs, tokens = super().__call__(batch)\n",
    "\n",
    "        # pad beginning and end of each protein due to legacy reasons\n",
    "        coords = [\n",
    "            F.pad(torch.tensor(cd), (0, 0, 0, 0, 1, 1), value=np.nan) #<---pad set to nan from np.inf\n",
    "            for cd, _ in coords_and_confidence\n",
    "        ]\n",
    "        confidence = [\n",
    "            F.pad(torch.tensor(cf), (1, 1), value=-1.)\n",
    "            for _, cf in coords_and_confidence\n",
    "        ]\n",
    "        coords = self.collate_dense_tensors(coords, pad_v=np.nan)\n",
    "        confidence = self.collate_dense_tensors(confidence, pad_v=-1.)\n",
    "        if device is not None:\n",
    "            coords = coords.to(device)\n",
    "            confidence = confidence.to(device)\n",
    "            tokens = tokens.to(device)\n",
    "        padding_mask = torch.isnan(coords[:,:,0,0])\n",
    "        coord_mask = torch.isfinite(coords.sum(-2).sum(-1))\n",
    "        confidence = confidence * coord_mask + (-1.) * padding_mask\n",
    "        return coords, confidence, strs, tokens, padding_mask\n",
    "\n",
    "    def from_lists(self, coords_list, confidence_list=None, seq_list=None, device=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            coords_list: list of length batch_size, each item is a list of\n",
    "            floats in shape L x 3 x 3 to describe a backbone\n",
    "            confidence_list: one of\n",
    "                - None, default to highest confidence\n",
    "                - list of length batch_size, each item is a scalar\n",
    "                - list of length batch_size, each item is a list of floats of\n",
    "                    length L to describe the confidence scores for the backbone\n",
    "                    with values between 0. and 1.\n",
    "            seq_list: either None or a list of strings\n",
    "        Returns:\n",
    "            coords: Tensor of shape batch_size x L x 3 x 3\n",
    "            confidence: Tensor of shape batch_size x L\n",
    "            strs: list of strings\n",
    "            tokens: LongTensor of shape batch_size x L\n",
    "            padding_mask: ByteTensor of shape batch_size x L\n",
    "        \"\"\"\n",
    "        batch_size = len(coords_list)\n",
    "        if confidence_list is None:\n",
    "            confidence_list = [None] * batch_size\n",
    "        if seq_list is None:\n",
    "            seq_list = [None] * batch_size\n",
    "        raw_batch = zip(coords_list, confidence_list, seq_list)\n",
    "        return self.__call__(raw_batch, device)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_dense_tensors(samples, pad_v):\n",
    "        \"\"\"\n",
    "        Takes a list of tensors with the following dimensions:\n",
    "            [(d_11,       ...,           d_1K),\n",
    "             (d_21,       ...,           d_2K),\n",
    "             ...,\n",
    "             (d_N1,       ...,           d_NK)]\n",
    "        and stack + pads them into a single tensor of:\n",
    "        (N, max_i=1,N { d_i1 }, ..., max_i=1,N {diK})\n",
    "        \"\"\"\n",
    "        if len(samples) == 0:\n",
    "            return torch.Tensor()\n",
    "        if len(set(x.dim() for x in samples)) != 1:\n",
    "            raise RuntimeError(\n",
    "                f\"Samples has varying dimensions: {[x.dim() for x in samples]}\"\n",
    "            )\n",
    "        (device,) = tuple(set(x.device for x in samples))  # assumes all on same device\n",
    "        max_shape = [max(lst) for lst in zip(*[x.shape for x in samples])]\n",
    "        result = torch.empty(\n",
    "            len(samples), *max_shape, dtype=samples[0].dtype, device=device\n",
    "        )\n",
    "        result.fill_(pad_v)\n",
    "        for i in range(len(samples)):\n",
    "            result_i = result[i]\n",
    "            t = samples[i]\n",
    "            result_i[tuple(slice(0, k) for k in t.shape)] = t\n",
    "        return result\n",
    "    \n",
    "\n",
    "class BatchConverter(object):\n",
    "    \"\"\"Callable to convert an unprocessed (labels + strings) batch to a\n",
    "    processed (labels + tensor) batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alphabet, truncation_seq_length: int = None):\n",
    "        self.alphabet = alphabet\n",
    "        self.truncation_seq_length = truncation_seq_length\n",
    "\n",
    "    def __call__(self, raw_batch: Sequence[Tuple[str, str]]):\n",
    "        # RoBERTa uses an eos token, while ESM-1 does not.\n",
    "        batch_size = len(raw_batch)\n",
    "        batch_labels, seq_str_list = zip(*raw_batch)\n",
    "        seq_encoded_list = [self.alphabet.encode(seq_str) for seq_str in seq_str_list]\n",
    "        if self.truncation_seq_length:\n",
    "            seq_encoded_list = [seq_str[:self.truncation_seq_length] for seq_str in seq_encoded_list]\n",
    "        max_len = max(len(seq_encoded) for seq_encoded in seq_encoded_list)\n",
    "        tokens = torch.empty(\n",
    "            (\n",
    "                batch_size,\n",
    "                max_len + int(self.alphabet.prepend_bos) + int(self.alphabet.append_eos),\n",
    "            ),\n",
    "            dtype=torch.int64,\n",
    "        )\n",
    "        tokens.fill_(self.alphabet.padding_idx)\n",
    "        labels = []\n",
    "        strs = []\n",
    "\n",
    "        for i, (label, seq_str, seq_encoded) in enumerate(\n",
    "            zip(batch_labels, seq_str_list, seq_encoded_list)\n",
    "        ):\n",
    "            labels.append(label)\n",
    "            strs.append(seq_str)\n",
    "            if self.alphabet.prepend_bos:\n",
    "                tokens[i, 0] = self.alphabet.cls_idx\n",
    "            seq = torch.tensor(seq_encoded, dtype=torch.int64)\n",
    "            tokens[\n",
    "                i,\n",
    "                int(self.alphabet.prepend_bos) : len(seq_encoded)\n",
    "                + int(self.alphabet.prepend_bos),\n",
    "            ] = seq\n",
    "            if self.alphabet.append_eos:\n",
    "                tokens[i, len(seq_encoded) + int(self.alphabet.prepend_bos)] = self.alphabet.eos_idx\n",
    "\n",
    "        return labels, strs, tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9434ab41",
   "metadata": {},
   "source": [
    "## 4. code to use coordinates info to encode features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39670450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder_output(model, alphabet, coords):\n",
    "    device = next(model.parameters()).device\n",
    "    batch_converter = CoordBatchConverter(alphabet)\n",
    "    batch = [(coords, None, None)]\n",
    "    coords, confidence, strs, tokens, padding_mask = batch_converter(\n",
    "        batch, device=device)\n",
    "    encoder_out = model.encoder.forward(coords, padding_mask, confidence,\n",
    "            return_all_hiddens=False)\n",
    "    # remove beginning and end (bos and eos tokens)\n",
    "    return encoder_out['encoder_out'][0][1:-1, 0]  # [L, D]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56876071",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f462d8d",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da1d44f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load following directory to the serach path: /home/yunyao/structural-evolution/bin\n",
    "import os\n",
    "import sys\n",
    "dir_path = '/home/yunyao/structural-evolution/bin'\n",
    "\n",
    "# Add the directory containing the modules to sys.path\n",
    "if dir_path not in sys.path:\n",
    "    sys.path.append(dir_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031be69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a DEEP mutation scanning library\n",
    "import argparse\n",
    "from dms_utils import deep_mutational_scan\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import esm\n",
    "from util import load_structure, extract_coords_from_structure\n",
    "import biotite.structure\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def get_native_seq(pdbfile, chain):\n",
    "    structure = load_structure(pdbfile, chain)\n",
    "    _ , native_seq = extract_coords_from_structure(structure)\n",
    "    return native_seq\n",
    "    \n",
    "def write_dms_lib(args):\n",
    "    '''Writes a deep mutational scanning library, including the native/wildtype (wt) of the \n",
    "    indicated target chain in the structure to an output Fasta file'''\n",
    "\n",
    "    sequence = get_native_seq(args.pdbfile, args.chain)\n",
    "    Path(args.outpath).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(args.dmspath, 'w') as f:\n",
    "        f.write('>wt\\n')\n",
    "        f.write(sequence+'\\n')\n",
    "        for pos, wt, mt in deep_mutational_scan(sequence):\n",
    "            assert(sequence[pos] == wt)\n",
    "            mut_seq = sequence[:pos] + mt + sequence[(pos + 1):]\n",
    "\n",
    "            f.write('>' + str(wt) + str(pos+1) + str(mt) + '\\n')\n",
    "            f.write(mut_seq + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a0c1a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test write_dms_lib\n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser(description='Generate a DMS library from a PDB file.')\n",
    "#     parser.add_argument('--pdbfile', type=str, required=True, help='Path to the PDB file.')\n",
    "#     parser.add_argument('--chain', type=str, required=True, help='Chain ID to extract sequence from.')\n",
    "#     parser.add_argument('--dmspath', type=str, required=True, help='Path to save the DMS library.')\n",
    "#     parser.add_argument('--outpath', type=str, required=True, help='Output path for the DMS library.')\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "#     write_dms_lib(args)\n",
    "\n",
    "pdbfile_path='/home/yunyao/structural-evolution/examples/7mmo_abc_fvar.pdb'\n",
    "chain_id='A'\n",
    "outpath='/home/yunyao/structural_evo_breakdown/test'\n",
    "dmspath = f'{outpath}/dms_lib.fasta'\n",
    "write_dms_lib(\n",
    "    argparse.Namespace(\n",
    "        pdbfile=pdbfile_path,\n",
    "        chain=chain_id,\n",
    "        dmspath=dmspath,\n",
    "        outpath=outpath\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4254af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "struct-evo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
